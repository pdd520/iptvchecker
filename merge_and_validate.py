import requests
import re
from typing import List, Tuple

# å®šä¹‰è¾“å…¥é“¾æ¥å’Œè¾“å‡ºæ–‡ä»¶
INPUT_URLS = [
    #"https://raw.githubusercontent.com/pdd520/iptv-api/refs/heads/master/output/result.m3u"
    "https://raw.githubusercontent.com/suxuang/myIPTV/refs/heads/main/ipv4.m3u"
]
OUTPUT_FILE = "emerged_output.m3u"
# éªŒè¯é“¾æ¥æ—¶çš„è¶…æ—¶è®¾ç½® (ç§’)
TIMEOUT = 5

# æ­£åˆ™è¡¨è¾¾å¼ç”¨äºåŒ¹é… M3U ä¸­çš„ EXTINF è¡ŒåŠå…¶ç´§éšçš„ URL
# (.+?) åŒ¹é… EXTINF ä¿¡æ¯
# (http.+?) åŒ¹é… http/https å¼€å¤´çš„ URLï¼Œè¿™æ˜¯æˆ‘ä»¬ä¸»è¦å…³æ³¨çš„æ’­æ”¾æºæ ¼å¼
M3U_PATTERN = re.compile(r'(#EXTINF:.*?\n)(http.*?)(?=\n#EXTINF|\n#EXTM3U|\Z)', re.DOTALL | re.IGNORECASE)


def download_and_extract(url: str) -> List[Tuple[str, str]]:
    """
    ä¸‹è½½ M3U å†…å®¹å¹¶æå– EXTINF ä¿¡æ¯å’Œ URLã€‚
    è¿”å›ä¸€ä¸ª (EXTINF_info, URL) å…ƒç»„çš„åˆ—è¡¨ã€‚
    """
    print(f"\nğŸ“¥ æ­£åœ¨ä¸‹è½½å’Œè§£æ: {url}")
    extracted_sources = []
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status() # æ£€æŸ¥ HTTP é”™è¯¯

        content = response.text
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…é¡¹
        matches = M3U_PATTERN.findall(content)
        
        for extinf_info, source_url in matches:
            # æ¸…ç† URLï¼Œå»é™¤å¯èƒ½çš„ç©ºç™½å­—ç¬¦
            source_url = source_url.strip()
            # EXTINF ä¿¡æ¯çš„æ ¼å¼æ¸…ç†
            extinf_info = extinf_info.strip()
            
            if source_url.startswith("http"):
                extracted_sources.append((extinf_info, source_url))

        print(f"âœ… æå–åˆ° {len(extracted_sources)} ä¸ªæ½œåœ¨æ’­æ”¾æºã€‚")
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ä¸‹è½½æˆ–è§£æå¤±è´¥ {url}: {e}")

    return extracted_sources

def check_url_status(url: str) -> bool:
    """
    æµ‹è¯• IPTV æ’­æ”¾æºæ˜¯å¦å¯ç”¨ã€‚
    """
    # éƒ¨åˆ†æ’­æ”¾æºå¯èƒ½éœ€è¦æ›´åƒæµè§ˆå™¨çš„ User-Agent
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        # ä½¿ç”¨ GET è¯·æ±‚å¹¶è®¾ç½® stream=Trueï¼Œåªä¸‹è½½å¤´éƒ¨ä¿¡æ¯å’Œå°‘é‡æ•°æ®ï¼Œæ›´å¿«
        response = requests.get(url, headers=headers, stream=True, timeout=TIMEOUT)
        
        # æ£€æŸ¥çŠ¶æ€ç æ˜¯å¦æˆåŠŸ (200-299)
        if 200 <= response.status_code < 300:
            return True
        else:
            print(f"    - HTTP Error {response.status_code}")
            return False
            
    except requests.exceptions.RequestException as e:
        # print(f"    - é”™è¯¯: {e}")
        return False
        
def main():
    all_sources = []
    valid_sources = []
    
    # 1. ä¸‹è½½å¹¶æå–æ‰€æœ‰æº
    for url in INPUT_URLS:
        all_sources.extend(download_and_extract(url))

    if not all_sources:
        print("ğŸ¤· æœªæ‰¾åˆ°ä»»ä½•æ’­æ”¾æºï¼Œé€€å‡ºã€‚")
        return

    print(f"\nğŸ’¡ æ€»å…±å‘ç° {len(all_sources)} ä¸ªæºï¼Œå¼€å§‹éªŒè¯...")
    
    # 2. éªŒè¯æ’­æ”¾æº
    # ä½¿ç”¨ set æ¥å­˜å‚¨éªŒè¯é€šè¿‡çš„ URLï¼Œé˜²æ­¢é‡å¤
    validated_urls = set()
    
    for i, (info, url) in enumerate(all_sources):
        # è¿›åº¦æ˜¾ç¤º (åªæ˜¾ç¤ºç™¾åˆ†æ¯”)
        if (i + 1) % 50 == 0 or (i + 1) == len(all_sources):
             print(f"    ... å·²éªŒè¯ {i + 1}/{len(all_sources)} ä¸ªæº ({((i + 1) / len(all_sources) * 100):.1f}%)")

        # è·³è¿‡å·²éªŒè¯è¿‡çš„é“¾æ¥ï¼Œæé«˜æ•ˆç‡
        if url in validated_urls:
            continue
            
        if check_url_status(url):
            valid_sources.append((info, url))
            validated_urls.add(url) # æ ‡è®°ä¸ºå·²éªŒè¯
            
    # 3. åˆå¹¶è¾“å‡º
    print(f"\nğŸ‰ éªŒè¯å®Œæˆ! æ‰¾åˆ° {len(valid_sources)} ä¸ªå¯ç”¨ä¸”ä¸é‡å¤çš„æ’­æ”¾æºã€‚")
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write("#EXTM3U\n")
        for info, url in valid_sources:
            # å†™å…¥ EXTINF ä¿¡æ¯
            f.write(info + "\n") 
            # å†™å…¥æ’­æ”¾æº URL
            f.write(url + "\n")

    print(f"âœ¨ å¯ç”¨æºå·²æˆåŠŸå†™å…¥ {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
